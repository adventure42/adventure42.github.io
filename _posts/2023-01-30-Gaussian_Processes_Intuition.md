---
layout: post                          # (require) default post layout
title: "Gaussian Processes - Intuition"   # (require) a string title
date: 2023-01-30       # (require) a post date
categories: [Statistics and Mathematics]          # (custom) some categories, but make sure these categories already exists inside path of `category/`
tags: [Statistics and Mathematics]                      # (custom) tags only for meta `property="article:tag"`
---

# Gaussian Processes

## Uncertainty

Machine learning methodë“¤ì€ ì£¼ì–´ì§„ "training data"ë¥¼ ê°€ì§€ê³  ê·¸ ì†ì— ë‹´ê¸´ patternì„ í•™ìŠµí•´ì„œ ë¯¸ë¦¬ ë³´ì§€ëª»í•œ ë¯¸ì§€ì˜ ì˜ì—­ì˜ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•œë‹¤. Gaussian processëŠ” ê·¸ methods ì¤‘ í•˜ë‚˜ì´ê³ , ë‹¤ë¥¸ methodë“¤ê³¼ëŠ” "uncertaintyì™€ì˜ ê´€ê³„" ê´€ì ì—ì„œ uniqueí•œ ì ì´ ìˆë‹¤.

ë¯¸ì§€ì˜ ì˜ì—­ (ì¦‰, machine learning ë¬¸ì œì˜ í•´ê²°ì„ í†µí•´ ì˜ˆì¸¡í•˜ë ¤ëŠ” uncertainty ì˜ì—­)ì€ ë°œìƒí•  ìˆ˜ ìˆëŠ” outcomesì™€ ì´ë“¤ì˜ ë°œìƒ í™•ë¥  ë¶„í¬ë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤. 

Uncertaintyë¥¼ í‘œí˜„í•˜ëŠ” probability distributionì€ ë‘ ê°€ì§€ë¡œ ë‚˜ë‰  ìˆ˜ ìˆë‹¤:

- discrete : finite number of possible outcomes (e.g., rolling a fair sided dice)
- continuous : outcome could be any real number (e.g., unknown height of my favorite character in a cartoon)

<br>

## Bayesian Inference

Bayes' inferenceëŠ” Bayes' ruleì„ statistical inferenceë¥¼ ìœ„í•´ ì‘ìš©í•˜ëŠ” ê²ƒì´ë‹¤.

Bayes rule = describe probability of an event, based on prior knowledge of conditions that might be related to the event.

Bayesian inferenceì˜ ê³µì‹ definition:

"One of the many applications of Bayes' theorem is Bayesian inference, a particular approach to statistical inference. When applied, the probabilities involved in the theorem may have different probability interpretations. With Bayesian probability interpretation, the theorem expresses how a degree of belief, expressed as a probability, should rationally change to account for the availability of related evidence."

Mathematically, Bayes' theoremì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤:

![](https://raw.githubusercontent.com/adventure42/adventure42.github.io/master/static/img/_posts/bayes_theorem.JPG)

Bayesian inferenceëŠ” ë‹¤ìŒ ë¬¸êµ¬ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤.

> Bayesian inference boils down to just a method for updating our beliefs about the world based on evidence that we observe.

ì—¬ê¸°ì—ì„œ "our belief"ëŠ” probability distributionìœ¼ë¡œ í‘œí˜„ëœë‹¤. Bayes' ruleì— ë”°ë¼ ì£¼ì–´ì§€ëŠ” ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ê¸°ì¡´ probability distributionì„ ì—…ë°ì´íŠ¸í•˜ê³  ë” ë‚˜ì€ "belief"ë¥¼ ë§Œë“¤ì–´ ë‚˜ì•„ê°„ë‹¤.

ì˜ˆì¸¡í•˜ê³  ì‹¶ì€ ê°’ì´ height(cm)ê³¼ ê°™ì´ continuous numeric valueì— ì†í•œ caseë¥¼ ì˜ˆì‹œë¡œ ë³¸ë‹¤ë©´, ë‹¤ìŒê³¼ ê°™ì´ probability distribution graphë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤. 

ë¹¨ê°„ìƒ‰ì´ ì²«ë²ˆì§¸ "belief"ì˜€ë‹¤ë©´, ìƒˆë¡­ê²Œ ì£¼ì–´ì§€ëŠ” ë°ì´í„°ë¥¼ ê°€ì§€ê³  íŒŒë€ìƒ‰ updateëœ "belief"ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ì˜ˆì¸¡í•˜ë ¤ëŠ” ê°’ì— ëŒ€í•œ probability distributionì´ë‹¤.

![](https://raw.githubusercontent.com/adventure42/adventure42.github.io/master/static/img/_posts/gaussian_process_prob_distribution_over_heights.JPG)

ì´ì™€ ë¹„ìŠ·í•œ ë§¥ë½ìœ¼ë¡œ, Gaussian processëŠ” ì˜ˆì¸¡í•˜ë ¤ëŠ” functionì— ëŒ€í•œ probability distributionì´ë‹¤.

> A Gaussian process is a probability distribution over possible functions

Gaussian processë¥¼ í™œìš©í•˜ì—¬ functionì— ëŒ€í•œ probability distributionì„ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  Bayes ruleì„ ì‚¬ìš©í•˜ì—¬ training dataì˜ í•™ìŠµì„ ê¸°ë°˜ìœ¼ë¡œ functionì˜ distributionì„ ì—…ë°ì´íŠ¸í•´ ë‚˜ì•„ ê°„ë‹¤. 

ì¶”ìƒì ì¸ ì˜ˆì‹œë¡œ ì„¤ëª…ì„ í•´ë³´ìë©´,

ë‹¤ìŒ graphë“¤ì€ unknown functionê³¼ ì´ë“¤ì˜ Gaussian processì˜  mean & standard deviationì„ ë³´ì—¬ì¤€ë‹¤. training dataë¥¼ í†µí•´ ë¶„ì„ ë° ì—…ë°ì´íŠ¸ê°€ ì§„í–‰ë˜ê¸° ì „ì˜ "prior belief"ë¼ê³  í•  ìˆë‹¤. ì˜¤ë¥¸ìª½ì—ëŠ” best guessë¡œ middle of real numberì¸ 0ì„ mean(ì¤‘ì‹¬)ìœ¼ë¡œ ë‘ê³ , ì™¼ìª½ì—ëŠ” ë„“ì€ ë²”ìœ„ì— í¼ì ¸ìˆëŠ” possible functionë“¤ì´ ê·¸ë ¤ì ¸ìˆë‹¤.

![](https://raw.githubusercontent.com/adventure42/adventure42.github.io/master/static/img/_posts/gaussian_process_unknown_function_mean_std.JPG)

ë§Œì•½ ë‹¤ìŒê³¼ ê°™ì´ "evidence" ì—­í• ì„ í•  training dataê°€ ì£¼ì–´ì§„ë‹¤ë©´, 

![](https://raw.githubusercontent.com/adventure42/adventure42.github.io/master/static/img/_posts/gaussian_process_known_data.JPG)

ë‹¤ìŒê³¼ ê°™ì´ Baye's ruleì„ í†µí•´ "prior belief"ë¥¼ updateí•  ìˆ˜ ìˆë‹¤. ì´ "posterior belief"ëŠ” ì£¼ì–´ì§„ training dataë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì°¾ì€ í›¨ì”¬ ë” narrowëœ possible functionë“¤ì´ë‹¤. possible functionì˜ meanì€ ì£¼ì–´ì§„ training dataì™€ ëª¨ë‘ interceptë¼ê³  standard deviationì„ ì–‘ìª½ ë ë¯¸ì§€ì˜ ì˜ì—­ìœ¼ë¡œ ê°ˆìˆ˜ë¡ ë” ë„“ì–´ì§„ë‹¤. 

> The updated Gaussian process is constrained to the possible functions that fit our training data.

![](https://raw.githubusercontent.com/adventure42/adventure42.github.io/master/static/img/_posts/gaussian_process_estimation.JPG)

<br>

## Gaussian Processì˜ ì¥ë‹¨ì 

1. ğŸ‘ Gaussian processì˜ ì¥ì ì€ ëª¨ë¥´ëŠ” ì˜ì—­ì„ ì¸ì‹í•˜ê³  ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.

> Gaussian processes know that they don't know. This is a direct consequence of Gaussian processes roots in probability and Bayesian inference.

ë‹¹ì—°í•˜ê²Œ ëŠê»´ì§ˆ ìˆ˜ë„ ìˆì§€ë§Œ, ë‹¤ë¥¸ machine learning methodë“¤ì€ ì´ ì¥ì ì´ ì—†ëŠ” ê²½ìš°ê°€ ë§ë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œ ë³´ì´ë“¯ì´, neural netê³¼ random forestì™€ëŠ” ë‹¤ë¥´ê²Œ, Gaussian processë¥¼ training dataì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ contour ìƒ‰ì´ ì˜…ì–´ì§„ë‹¤ (ì¦‰, ì£¼ì–´ì§„ training dataì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ uncertaintyê°€ ì¦ê°€í•œë‹¤.)

ë†’ì€ classification accuracyë¡œ ì•Œë ¤ì§„ neural netê³¼ random forestëŠ” training dataì—ì„œ ë©€ì–´ì ¸ë„ ë†’ì€ certaintyë¥¼ ìœ ì§€í•œë‹¤. ì´ëŸ° methodë“¤ì€ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ê°€ì§€ê³ ìˆì§€ë§Œ, ì¢…ì¢… ë¯¸ì§€ì˜ ì˜ì—­ì—ì„œ "adversarial examples"ë¥¼ ë°œìƒì‹œí‚¤ëŠ” ë¬¸ì œë¥¼ ì¼ìœ¼í‚¨ë‹¤. 

> Adversarial examples: when powerful classifiers give very wrong predictions for strange reasons.

Gaussian processì˜ ì¥ì ì´ outputì— ëŒ€í•œ certaintyë¥¼ ë†’ì´ê¸°ë•Œë¬¸ì—, adversarial caseë¡œ ë¶€í„°ëŠ” ë©€ë¦¬, identity verificationì´ë‚˜ security critical use caseì—ëŠ” ë” ì ì ˆí•œ methodê°€ ë˜ë„ë¡ í•œë‹¤. 

![](https://raw.githubusercontent.com/adventure42/adventure42.github.io/master/static/img/_posts/comparison_gp_nn_rf.JPG)

<br>

2. ğŸ‘ Gaussian processëŠ” kernelì˜ ì„ íƒì— ë”°ë¼ "prior belief"ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆë‹¤.

> Gaussian processes let you incorporate expert knowledge via the choice of kernel

Kernelì„ ì–´ë–»ê²Œ ì„¤ì •í•˜ëƒì— ë”°ë¼ì„œ fitted functionì„ ë‹¤ì–‘í•œ ëª¨ì–‘ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆë‹¤. Uncertainty ì˜ì—­ì— ëŒ€í•œ GPì˜ generalizationì€ kernelë¡œ ì¸í•´ ê±°ì˜ ê²°ì • ëœë‹¤ê³  ë³´ë©´ ëœë‹¤. 

<br>

3. ğŸ‘ Gaussian processëŠ” Computationally expensive

Gaussian processëŠ” non-parametric methodì´ë‹¤. Parametric approachëŠ” "a set of numbers"ì— training dataë¡œ ë¶€í„° ì–»ì€ ì •ë³´ë¥¼ ë„£ì„ ìˆ˜ ìˆë‹¤. (e.g. linear regressionì˜ ê²½ìš°ì—ëŠ” ë‹¨, ë‘ ê°œì˜ numbers - the slope and the intercept - ì— approximate functionì„ ë§Œë“¤ ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ë„£ëŠ”ë‹¤.) Parametricì˜ ê²½ìš°ì—ëŠ” ë¯¸ì§€ì˜ ì˜ì—­ì— ëŒ€í•œ ì˜ˆì¸¡ì„ êµ¬í•˜ëŠ” inference ë‹¨ê³„ì—ì„œ ì´ "set of numbers"ë§Œ ìˆìœ¼ë©´ predictionì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤. (after training, the cost of making predictions is dependent only on the number of parameters.)

Non-parametricì˜ ê²½ìš°ì—ëŠ” training data ì „ë¶€ë¥¼ ê³ ë ¤í•´ì•¼ predictionì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤. This means not only that the training data had to be kept at inference time, but also means that the computational cost of predictions scale with the number of training samples. 

Deep learningì´ dominantí•œ ë¶„ì•¼ì—ì„œë„ Gaussian processë¥¼ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ì—°êµ¬ê°€ ì§„í–‰ë˜ê³  ìˆë‹¤. Deep & convolutional Gaussian processë¥¼ í†µí•´ high-dimensional and image dataë¥¼ ì²˜ë¦¬í•˜ê³ , large datasetì—ë„ í™œìš© ë  ìˆ˜ ìˆë„ë¡ sparse and minibatch Gaussian processë¡œ scalabilityë¥¼ ë†’ì´ëŠ” ë°©ì•ˆ, ë“±ì´ ìˆë‹¤.

<br>

<br>

# References 

1. An Intuitive Guide to Gaussian Processes by Oscar Knagg : https://towardsdatascience.com/an-intuitive-guide-to-gaussian-processes-ec2f0b45c71d 
1. The Gaussian processes framework in Python https://github.com/SheffieldML/GPy
1. Baye's theorem : https://en.wikipedia.org/wiki/Bayes%27_theorem
