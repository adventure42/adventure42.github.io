---
layout: post                          # (require) default post layout
title: "RNN"                   # (require) a string title
date: 2021-12-22       # (require) a post date
categories: [machinelearning]          # (custom) some categories, but makesure these categories already exists inside path of `category/`
tags: [RNN]                      # (custom) tags only for meta `property="article:tag"`

---





# RNN (recurrent neural network)

times series data(data가 sequence 형태이거나 data내의 순서가 의미가 있을때)를 학습해서 예측값을 찾으려 할때에는 RNN(순환 신경망)을 사용한다. RNN은 고정된 길이의 입력이 아닌 임이의 길이를 가진 sequence를 다룰 수 있다. (e.g., 문장, 문서, 오디오 샘플, 등) 기본적으로 RNN이 사용될 수 있지만, 제한적인 단기 기억을 다루도록 기능을 향상하기 위해서는 LSTM과 GRU cell을 사용해서 모델을 확장할 수 있다.

<br>

<br>

## RNN기본 개념

RNN은 feedforward 신경망과 비슷하지만, 각 neuron을 보면 뒤쪽으로 순환하는 연결이 있다는 점이 다르다. 다음 그림과 같이 neuron A는 입력 x를 받아서 출력 h를 만드는것 외에도 자신에게 보내는 출력을 만든다. 

![RNN_neuron](https://raw.githubusercontent.com/adventure42/adventure42.github.io/master/static/img/_posts/RNN_neuron.png)

time step t에서 순환 neuron의 출력은 이전 time step의 모든 입력에 대한 함수이기때문에 일종의 메모리 형태라고 할 수 있다. time step에 걸쳐서 어떤 상태를 보존하는 신경망의 구성 요소를 **memory cell**이라고 한다.  

여기에서 cell의 상태를 h로 나타내는데, hidden 은닉을 의미한다. time step t에서의 cell 상태를 h_t로 표현하는데, 이는 그 time step의 입력과 이전 time step의 상태에 대한 함수이다. 즉, 순환 neuron은 각 time step(또는 frame) t 마다 x_t와 이전 time step의 출력인 h_ (t-1)를 입력으로 받는것이다. 

활성화 함수로는 ReLU대신 tanh를 많이 사용한다. 각 순환 neuron은 두개의 가중치를 가진다. 하나는 입력 x_t를 위한것이고 다른 하나는 이전 time step의 출력 h_ (t-1)를 위한 것이다. 이 가중치를 w_xh, w_hh로, 편향을 b_h로 표현해서 다음과 같이 recurrent neuron의 출력을 표현할 수 있다.

<img src="https://raw.githubusercontent.com/adventure42/adventure42.github.io/master/static/img/_posts/RNN_basic.png" alt="RNN basic" style="zoom:50%;" />

```python
# 코드로 표현한 neuron하나로 구성된 하나의 층을 가진 간단한 RNN
model = keras.models.Sequential([
    keras.layers.SimpleRNN(1, input_shape=[None,1])
])
```

<br>

RNN은 입력 sequence를 받아서 출력 sequence를 만들 수 있다. 예를 들어서 sequence-to-sequence network는 주식가격같은 시계열 데이터를 예측하는데에 유용하다. (최근 N일치의 주식 가격을 주입하면 network는 각 입력값보다 하루 앞선 가격을(N-1일 전부터 내일까지) 출력한다. 

sequence-to-vector network은 입력 시퀀스를 network에 주입하고, 마지막을 제외한 모든 출력을 무시할수도 있다. 예를 들어서 영화 리뷰에 있는 연속된 단어를 주입하면 network는 감성점수를 출력한다.

반대로 각 time step에서 하나의 입력 vector을 반복적으로 network에 입력하고, 하나의 sequence를 출력하는 vector-to-sequence network도 존재한다. 예를 들어 이미지(또는 CNN의 출력)을 입력하여 이미지에 대한 caption을 출력할 수 있다. 

encoder라고 불리는 sequence-to-vector network뒤에 decoder라고 불리는 vector-to-sequence network을 연결하면 "encoder-decoder"라고 부르는 구조가 만들어지고 예를 들어서 한 언어의 문장을 다른 언어로 번역하는데에 활용될 수 있다. 한 언어의 문장을 network에 입력하면, encoder는 이 문장을 하나의 vector로 표현하고, 이 vector는 decoder를 통해 다른 언어의 문장으로 decoding된다. (이런 방식이 하나의 sequnce-to-sequence network을 사용해서 한단어씩 번역하는 것 보다 훨씬 더 잘 번역을 수행할 수 있다.) 

<br>

<br>

## RNN 훈련

RNN을 time step 순서대로 펼치고, 보통의 역전파를 사용해서 훈련을 진행할 수 있다. 이 방법은 BPTT(back propagation through time)이라고 불린다.

기존 역전파가 진행되는 것과 같이 먼저 정방향패스가 펼쳐진 network를 통과 하고 비용함수를 사용해서 출력 sequence를 평가한다. 그리고 비용함수의 gradient는 펼쳐진 network를 따라 역방향으로 전파된다. (이 비용함수는 일부 출력을 무시할 수 도있다. 예를 들어서 network의 마지막 출력 Y_n, Y_ (n-1), Y_ (n-2) 만 사용해서 비용함수를 계산하고 gradient는 이 세 개의 출력을 거치고 Y_0, Y_1, ...등의 앞의 출력들은 거치지 않는다.) model parameter들은 BPTT동안 계산된 gradient를 사용해서 update된다. 각 time step마다 같은 매개변수 W와 b가 사용되기때문에 역전파가 진행되면 모든 time step에 결쳐 합산된다. 

<br>

<br>

## 시계열 예측하기

모든 data가 time step마다 하나 이상의 값을 가진 sequence의 경우 time series하고 부른다. time step마다 하나의 값을 가지는 경우는 univariate(단변량) time series라고 하고, 여러값을 가지는 경우는 multivariate(다변량) time series라고 한다. 

time series data를 기반으로 미래를 예측하는 것을 forecasting이라고 부르고, 반대로 과거에 누락되었던 값을 예측하는 것을 imputation(값 대체)라고 한다.

시계열을 다룰때 입력 특성은 일반적으로 크기의 3D 배열로 나타난다. [batch_size, time step수, 차원 수] 단변량 시계열은 dimensionality가 1 이고 다변량 시계열은 1이상이된다. 그래서 단변량인 경우 [batch size, tim step수, 1]크기의 numpy 배열을 반환한다. 

<br>

<br>

## 기준 성능

각 시계열의 마지막 값을 그대로 예측하는 것을 naive forecasting (순진한 예측)이라고 한다. 이 성능보다 더 좋은 성능을 내는것이 매우 어렵기때문에, naive forecasting을 기준으로 모델이 잘 작동하는 지 확인 할 수 있다. 

```python
n_steps = 50
batch_size=10000
#generate_time_series함수로 n_steps길이의 시계열 데이터 생성
series = generate_time_series(batch_size, n_steps+1)
X_train, y_train = series[:7000, :n_steps], series[:7000,-1]

y_pred = X_valid[:,-1]
np.mean(keras.losses.mean_squared_error(y_valid, y_pred))
#0.02 수준의 평균제곱오차가 확인된다.
```

<br>

다른 방법으로 다음과 같이 fully connected network을 사용할 수 있다

```python
model = keras.models.Sequential([
    keras.layers.Flatten(input_shape=[50,1]),
    keras.layers.Dense(1)
])
```

<br>

<br>

## 간단한 RNN (simpleRNN)

위에서 recurrent neuron의 모형으로 보여진것과 같이 simple RNN는하나의 neuron이 있는 하나의 층으로 구성되어있고 기본적으로 활성화 함수로 tanh 함수를 사용한다. 

```python
model = keras.models.Sequential([
    keras.layers.SimpleRNN(1, input_shape=[None,1])
])
```

초기 상태 h_init을 0으로 설정하고 첫번째 time step x_0와 함께 하나의 recurrent neuron으로 전달된다. neuron은 이 값의 가중치 합을 계산하고 tanh 활성화 함수를 적용해서 결과를 만들어서 첫번째 y_0를 출력한다. simpleRNN에서는 이 출력이 새로운 상태 h_0이 된다. 그리고 이 새로운 상태는 다음 time step의 입력값인 x_1과 함께 동일한 recurrent neuron으로 전달된다. 이렇게 계속 마지막 time step까지 반복된다. n_step=50의 경우, 마지막 time step의 층이 마지막 값 y_49를 출력한다.

위에서 fully connected network을 사용했을때에는 (assuming 간단한 linear model) 총 51개의 parameter가 사용한다. (50개의 neuron 각각 하나의 parameter + bias = 51개)

simpleRNN의 경우 recurrent neuron은 입력과 은닉상태 차원 (simpleRNN에서는 neuron수 = 층수) 마다 하나의 parameter를 사용하고 편향이 있기때문에 총 3개의 parameter를 가지고있다. 

<br>

<br>

## 심층 RNN (deepRNN)

<img src="https://raw.githubusercontent.com/adventure42/adventure42.github.io/master/static/img/_posts/deepRNN.PNG" alt="deepRNN" style="zoom: 67%;" />

일반적으로 RNN은 이렇게 여러 층을 쌓아서 심층RNN으로 활용한다. 

```python
model = keras.models.Sequential([
    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None,1]),
    keras.layers.SimpleRNN(20),
    keras.layers.Dense(1)
])
```

tbc